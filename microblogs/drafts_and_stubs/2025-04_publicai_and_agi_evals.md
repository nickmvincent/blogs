# AI Evals Should be as Public as Possible

A proposal that I believe many in the public AI orbit can rally around -- regardless of their stance on AI, whether they see themselves as more concerned with safety, ethics, or acceleration -- is that the evaluation process by which AI capabilities are "proven" should be as public as possible. In other words, if we're going to build a "test" that proves "AGI has been achieved" that should be extremely public.

While labs can continue to claim they need to protect training data details, algorithmic tweaks, engineering decisions, etc. on the basis of competition, they are at the same time continuing to release model cards and signaling an interest to communicating capabilities to the public.

So, we should lean into collaboration between AI labs and NIST, UK AISI, Canadian AI institutes, news orgs like Transluce, etc. and make evaluation practices and "definitional practices" *way* more public. 